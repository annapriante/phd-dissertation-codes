{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This code allows to extract URLs from tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "import urllib.parse\n",
    "\n",
    "def get_forward_online(url, history=None):\n",
    "    if (history is None):\n",
    "        history = list()\n",
    "    if (len(url) < 2):\n",
    "        return (url, history)\n",
    "    sys.stderr.write(\"GET \" + url + \"\\n\")\n",
    "    try:\n",
    "        head = requests.head(url, timeout=10)\n",
    "    except Exception:\n",
    "        sys.stderr.write(\"Warning: Exception for url: \" + url + \"\\n\")\n",
    "        return (url, history)\n",
    "    code = head.status_code\n",
    "    if (code == 301 or code == 302):\n",
    "        headers  = head.headers\n",
    "        if ('Location' in headers):\n",
    "            location = head.headers['Location']\n",
    "            location = urllib.parse.urljoin(url, location)\n",
    "            history.append(url)\n",
    "            return get_forward_online(location, history)\n",
    "        else:\n",
    "            raise ValueError('No Location header')\n",
    "    else:\n",
    "        if (code != 200):\n",
    "            sys.stderr.write(\"Warning: \" + str(code) + \" for url: \" + url + \"\\n\")\n",
    "    return (url, history)\n",
    "\n",
    "\n",
    "def add_protocol(url):\n",
    "    if (len(url) > 1 and not(url.startswith('http'))):\n",
    "        url = 'http://' + url\n",
    "    return url\n",
    "\n",
    "\n",
    "def print_csv(id, url1, hist1, url2, hist2, url3, hist3):\n",
    "    list1 = \" \".join(hist1)\n",
    "    list2 = \" \".join(hist2)\n",
    "    list3 = \" \".join(hist3)\n",
    "    print (\";\".join(list((id, url1, list1, url2, list2, url3, list3))))\n",
    "\n",
    "print ('TweetID;URL1;HISTORY1;URL2;HISTORY2;URL3;HISTORY3;')\n",
    "\n",
    "h_cache = dict()\n",
    "u_cache = dict()\n",
    "\n",
    "def add_to_cache(url, hist):\n",
    "    res = hist.split()\n",
    "    if (len(res) > 1):\n",
    "        u_cache[res[0]] = url\n",
    "        h_cache[res[0]] = res\n",
    "\n",
    "def get_forward(url):\n",
    "    url = add_protocol(url)\n",
    "    if (url in h_cache):\n",
    "        hist = h_cache[url]\n",
    "        res  = u_cache[url]\n",
    "        sys.stderr.write(\"CACHED: \" + url + \"\\n\")\n",
    "        return (res, hist)\n",
    "    else:\n",
    "        return get_forward_online(url)\n",
    "\n",
    "try:\n",
    "    with open('URLS_checked.csv', 'r') as csvfile:\n",
    "        for row in csvfile:\n",
    "            (id, url1, hist1, url2, hist2, url3, hist3) = row.split(';', 6)\n",
    "            add_to_cache(url1, hist1)\n",
    "            add_to_cache(url2, hist2)\n",
    "            add_to_cache(url3, hist3)\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "with open('URLS_to_check.csv', 'r') as csvfile:\n",
    "    for row in csvfile:\n",
    "        (id, url1, url2, url3, rest) = row.split(';', 4)\n",
    "        if (id != \"TweetID\"):\n",
    "            (url1, hist1) = get_forward(url1)\n",
    "            (url2, hist2) = get_forward(url2)\n",
    "            (url3, hist3) = get_forward(url3)\n",
    "            print_csv(id, url1, hist1, url2, hist2, url3, hist3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
